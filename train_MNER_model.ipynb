{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "j1pvk8dIIzi3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9188565a-e498-4724-9f23-b8688372062b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2W7-nj0iDLXa",
        "outputId": "a6ceeeb6-3705-4700-eebd-9341ea982955"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJZfMO89G4oA",
        "outputId": "46394d7a-5d50-406b-8ebd-a5c1eea7a490"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.17.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: torch==2.2.1 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.2.1+cu121)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.1->torchvision) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.2.1->torchvision) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2.1->torchvision) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cw0ORR1mHALT",
        "outputId": "6d197957-dd05-449f-b6f6-33144dab8b56"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgjs3H4_HCkv",
        "outputId": "d83fda93-480a-4089-daa2-ed48e084375c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchcrf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-y95sTNrLdBI",
        "outputId": "71503ae9-a00e-4159-a071-755a895c72a0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchcrf in /usr/local/lib/python3.10/dist-packages (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchcrf) (1.25.2)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from torchcrf) (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchcrf) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchcrf) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchcrf) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchcrf) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchcrf) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchcrf) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchcrf) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchcrf) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchcrf) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchcrf) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchcrf) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchcrf) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchcrf) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchcrf) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchcrf) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchcrf) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchcrf) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchcrf) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.0.0->torchcrf) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->torchcrf) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->torchcrf) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-crf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9A9ZgurEz6p",
        "outputId": "1a4853f7-328b-40da-837e-9888d34c5e9c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch-crf in /usr/local/lib/python3.10/dist-packages (0.7.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQ8sUeYLUfDv",
        "outputId": "6393a4aa-9853-49f3-85f7-9f58e86ea137"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchvision.models import resnet152\n",
        "from transformers import BertModel, BertTokenizer\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import os\n",
        "import torchvision.transforms\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertConfig, BertPreTrainedModel\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "from sklearn.metrics import f1_score\n",
        "from torch.optim import Adam\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torchcrf import CRF\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "S4-gu12BEvSe"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/drive/MyDrive/twitter2015 /content/data/"
      ],
      "metadata": {
        "id": "f_TFRcTiI3uO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/drive/MyDrive/twitter2015_images /content/data/"
      ],
      "metadata": {
        "id": "m6YyyIBVI4NI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/drive/MyDrive/twitter2017 /content/data/"
      ],
      "metadata": {
        "id": "PVa8H-U9I4lB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/drive/MyDrive/twitter2017_images /content/data/"
      ],
      "metadata": {
        "id": "oz3hxSzkI464"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TwitterDataset(Dataset):\n",
        "    def __init__(self, data_folder, img_folder, tokenizer, transform, file_name):\n",
        "        super().__init__()\n",
        "        self.data_lines = []\n",
        "        self.img_folder = img_folder\n",
        "        self.tokenizer = tokenizer\n",
        "        self.transform = transform\n",
        "\n",
        "        # Load data from the specified file\n",
        "        file_path = os.path.join(data_folder, file_name)\n",
        "        with open(file_path, 'r', encoding=\"utf8\") as file:\n",
        "            img_id = None\n",
        "            text = []\n",
        "            labels = []\n",
        "\n",
        "            counter = 0\n",
        "            for line in file:\n",
        "                if counter == 100:\n",
        "                  break\n",
        "                if line.strip() == '' and img_id is not None:  # save previous instance\n",
        "                    self.data_lines.append((img_id, text, labels))\n",
        "                    img_id = None  # Reset for the next image\n",
        "                    text = []\n",
        "                    labels = []\n",
        "                elif line.startswith('IMGID:'):\n",
        "                    img_id = line.strip().split(':')[1] + '.jpg'  # New image id\n",
        "                else:\n",
        "                    parts = line.strip().split('\\t')\n",
        "                    if len(parts) == 2:\n",
        "                        text.append(parts[0])\n",
        "                        labels.append(parts[1])\n",
        "\n",
        "                counter+=1\n",
        "\n",
        "            # Save last instance if not empty\n",
        "            if img_id is not None:\n",
        "                self.data_lines.append((img_id, text, labels))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_lines)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_id, text, labels = self.data_lines[idx]\n",
        "        image_path = os.path.join(self.img_folder, img_id)\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        text = ' '.join(text)\n",
        "        labels = [self.label_to_idx(label) for label in labels]  # Convert labels to indices\n",
        "\n",
        "        inputs = self.tokenizer(text, padding='max_length', max_length=128, truncation=True,return_tensors=\"pt\")\n",
        "        image = self.transform(image)\n",
        "        labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "        return inputs.input_ids.squeeze(0), inputs.attention_mask.squeeze(0), image, labels\n",
        "\n",
        "    @staticmethod\n",
        "    def label_to_idx(label):\n",
        "        # Define your label to index mapping based on your dataset's labels\n",
        "        label_map = {\n",
        "            'O': 0,\n",
        "            'B-LOC': 1, 'I-LOC': 2,\n",
        "            'B-PER': 3, 'I-PER': 4,\n",
        "            'B-ORG': 5, 'I-ORG': 6\n",
        "        }\n",
        "        return label_map.get(label, 0)  # Convert unrecognized labels to 'O'\n"
      ],
      "metadata": {
        "id": "yU2DrCT9E1je"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BertCrossAttention(nn.Module):\n",
        "    \"\"\"Implements cross-attention between two different modalities using a decoder layer.\"\"\"\n",
        "\n",
        "    def __init__(self, config, num_layers):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            nn.TransformerDecoderLayer(d_model=config.hidden_size, nhead=config.num_attention_heads)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, query, key, mask=None):\n",
        "        output = query\n",
        "        for layer in self.layers:\n",
        "            output = layer(output, key, tgt_key_padding_mask=mask)\n",
        "        return output\n",
        "\n",
        "\n",
        "class MTCCMBertForMMTokenClassificationCRF(BertPreTrainedModel):\n",
        "    def __init__(self, config, num_labels, add_context_aware_gate=False, use_dynamic_cross_modal_fusion=False):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = num_labels\n",
        "        self.bert = BertModel(config)\n",
        "        self.resnet = resnet152(pretrained=True)\n",
        "        self.resnet.fc = nn.Identity()  # Adapt ResNet to remove the final fully connected layer\n",
        "\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.vismap2text = nn.Linear(2048, config.hidden_size)\n",
        "\n",
        "        self.txt2img_attention = BertCrossAttention(config, num_layers=1)\n",
        "        self.img2txt_attention = BertCrossAttention(config, num_layers=1)\n",
        "\n",
        "        self.add_context_aware_gate = add_context_aware_gate\n",
        "        # Initialize the visual filter gate\n",
        "        if add_context_aware_gate:\n",
        "            self.visual_gate = ContextAwareGate(config.hidden_size, 768)\n",
        "\n",
        "        if use_dynamic_cross_modal_fusion:\n",
        "            self.dynamic_attention = DynamicAttentionModule(config.hidden_size)\n",
        "\n",
        "        self.classifier = nn.Linear(config.hidden_size * 2, num_labels)\n",
        "        self.crf = CRF(num_labels, batch_first=True)\n",
        "        self.init_weights()\n",
        "\n",
        "        self.use_dynamic_cross_modal_fusion = use_dynamic_cross_modal_fusion\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, visual_embeds, labels=None):\n",
        "        # Text feature extraction\n",
        "        text_outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
        "        text_features = self.dropout(text_outputs.last_hidden_state)\n",
        "\n",
        "        # Image feature extraction with ResNet-152\n",
        "        visual_features = self.resnet(visual_embeds)  # Assuming visual_embeds is [batch_size, 3, 224, 224]\n",
        "        visual_features = visual_features.view(visual_features.size(0), -1)  # Flatten the output of the ResNet\n",
        "        visual_features = self.vismap2text(visual_features)  # Transform to match BERT hidden size\n",
        "        visual_features = visual_features.unsqueeze(1).expand(-1, text_features.size(1),\n",
        "                                                              -1)  # Expand to match text sequence length\n",
        "\n",
        "        if self.add_context_aware_gate:\n",
        "            visual_features = self.visual_gate(text_features, visual_features)\n",
        "\n",
        "        # Cross-modal attention\n",
        "        if self.use_dynamic_cross_modal_fusion:\n",
        "            attended_text, attended_visuals = self.dynamic_attention(text_features, visual_features)\n",
        "            combined_features = torch.cat([attended_text, attended_visuals], dim=-1)\n",
        "        else:\n",
        "            txt_attended_visuals = self.txt2img_attention(text_features, visual_features)\n",
        "            img_attended_text = self.img2txt_attention(visual_features, text_features)\n",
        "            combined_features = torch.cat([txt_attended_visuals, img_attended_text], dim=-1)\n",
        "\n",
        "        logits = self.classifier(combined_features)\n",
        "\n",
        "        # crf processing\n",
        "        if labels is not None:\n",
        "            # Ensure labels and logits have the same sequence length\n",
        "            labels = torch.where(labels == -100, torch.zeros_like(labels), labels)\n",
        "\n",
        "            seq_length = logits.size(1)\n",
        "            if labels.size(1) < seq_length:\n",
        "                padding_size = seq_length - labels.size(1)\n",
        "                # Use a valid label index for padding, e.g., 0\n",
        "                labels_padded = torch.full((labels.size(0), padding_size), fill_value=0, dtype=torch.long,\n",
        "                                           device=labels.device)\n",
        "                labels = torch.cat([labels, labels_padded], dim=1)\n",
        "\n",
        "                # Adjust attention_mask to cover only the non-padded areas\n",
        "                attention_mask_padded = torch.zeros((attention_mask.size(0), seq_length), dtype=torch.uint8,\n",
        "                                                    device=attention_mask.device)\n",
        "                attention_mask_padded[:, :attention_mask.size(1)] = attention_mask\n",
        "                attention_mask = attention_mask_padded\n",
        "\n",
        "            # CRF loss calculation\n",
        "            loss = -self.crf(logits, labels, mask=attention_mask.byte(), reduction='mean')\n",
        "            return loss\n",
        "        else:\n",
        "            return self.crf.decode(logits, mask=attention_mask.byte())\n",
        "\n",
        "\n",
        "class ContextAwareGate(nn.Module):\n",
        "    def __init__(self, text_dim, visual_dim):\n",
        "        super().__init__()\n",
        "        # network to calculate threshold\n",
        "        self.threshold_network = nn.Sequential(\n",
        "            nn.Linear(text_dim, 1),  # Averaging text features to a single value\n",
        "            nn.Sigmoid()  # Ensure the threshold is between 0 and 1\n",
        "        )\n",
        "        self.gate = nn.Sequential(\n",
        "            nn.Linear(visual_dim * 2, visual_dim),  # Combine visual and transformed text features\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(visual_dim, visual_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, text_features, visual_features):\n",
        "        combined_features = torch.cat([text_features, visual_features], dim=-1)\n",
        "        # Compute gating values\n",
        "        gate_values = self.gate(combined_features)\n",
        "        # Apply the gate to the visual features only\n",
        "        text_mean = torch.mean(text_features, dim=1)\n",
        "        update_threshold = self.threshold_network(text_mean).squeeze()  # Ensuring scalar output per batch item\n",
        "\n",
        "        # if the threshold is larger than certain value apply combined features to the visual features\n",
        "        update_threshold_expanded = update_threshold.unsqueeze(-1).unsqueeze(-1)\n",
        "        update_mask = (gate_values > update_threshold_expanded).float()\n",
        "\n",
        "        # Apply the gate to the visual features conditionally\n",
        "        updated_visual_features = visual_features * (1 - update_mask) + (visual_features * gate_values) * update_mask\n",
        "\n",
        "        return updated_visual_features\n",
        "\n",
        "\n",
        "class DynamicAttentionModule(nn.Module):\n",
        "    def __init__(self, feature_dim):\n",
        "        super().__init__()\n",
        "        self.text_weight_predictor = nn.Sequential(\n",
        "            nn.Linear(feature_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.visual_weight_predictor = nn.Sequential(\n",
        "            nn.Linear(feature_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, text_features, visual_features):\n",
        "        text_weights = self.text_weight_predictor(text_features).expand_as(text_features)\n",
        "        visual_weights = self.visual_weight_predictor(visual_features).expand_as(visual_features)\n",
        "        attended_text = text_features * text_weights\n",
        "        attended_visuals = visual_features * visual_weights\n",
        "        return attended_text, attended_visuals\n"
      ],
      "metadata": {
        "id": "2zTOt7PrFdpr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    input_ids, attention_masks, images, labels = zip(*batch)\n",
        "\n",
        "    # Pad the sequences\n",
        "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
        "    attention_masks = pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
        "\n",
        "    # Ensure the first timestep of each mask is on\n",
        "    attention_masks[:, 0] = 1\n",
        "\n",
        "    # Stack images and pad labels\n",
        "    images = torch.stack(images)\n",
        "    labels = pad_sequence(labels, batch_first=True, padding_value=-100)  # Assuming -100 is your ignore index for labels\n",
        "\n",
        "    return input_ids, attention_masks, images, labels\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, val_loader, optimizer, scheduler, num_epochs=10):\n",
        "    best_f1 = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(\"Training for epoch \", str(epoch))\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        train_progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False)\n",
        "\n",
        "\n",
        "        for inputs, masks, images, labels in train_progress_bar:\n",
        "            optimizer.zero_grad()\n",
        "            loss = model(inputs, masks, images, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "        val_loss, val_f1 = evaluate_model(model, val_loader)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_train_loss}, Val Loss: {val_loss}, Val F1: {val_f1}\")\n",
        "\n",
        "        # Save the model if the validation F1 score is the best we've seen so far.\n",
        "        save_path = \"/content/output/epoch_\" + str(epoch+1) + \"_valf1_\" + str(val_f1) + \".pth\"\n",
        "        if val_f1 > best_f1:\n",
        "            best_f1 = val_f1\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            print(\"Saved best model\")\n",
        "\n",
        "\n",
        "def evaluate_model(model, data_loader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        validation_progress_bar = tqdm(data_loader, desc='Validating', leave=False)\n",
        "\n",
        "        for inputs, masks, images, labels in validation_progress_bar:\n",
        "            loss = model(inputs, masks, images, labels)\n",
        "            total_loss += loss.item()\n",
        "            predictions = model(inputs, masks, images)  # Decoding without labels returns predictions\n",
        "\n",
        "            # Flatten the labels for evaluation\n",
        "            labels_flattened = labels.view(-1).cpu().numpy()  # Flatten labels and move to CPU\n",
        "\n",
        "            # Flatten predictions and convert to numpy array if predictions are a list\n",
        "            if isinstance(predictions, list):\n",
        "                # Convert list of lists to a flat list if necessary\n",
        "                flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "                predictions = torch.tensor(flat_predictions)  # Convert to tensor\n",
        "            else:\n",
        "                predictions = predictions.view(-1)  # Flatten predictions\n",
        "\n",
        "            predictions = predictions.cpu().numpy()  # Convert to numpy for f1 score calculation\n",
        "\n",
        "            # Filter out the padding index (-100) from labels and corresponding predictions\n",
        "            valid_indices = labels_flattened != -100\n",
        "            valid_labels = labels_flattened[valid_indices]\n",
        "            valid_predictions = predictions[valid_indices]\n",
        "\n",
        "            all_preds.extend(valid_predictions)\n",
        "            all_labels.extend(valid_labels)\n",
        "\n",
        "    print(all_labels)\n",
        "    print(all_preds)\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')  # Calculate macro F1 Score\n",
        "    return avg_loss, f1"
      ],
      "metadata": {
        "id": "kEZJdcOQFeMj"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_folder = 'data/twitter2015'  # Update accordingly\n",
        "img_folder = 'data/twitter2015_images'  # Update accordingly\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.Resize((224, 224)),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "train_dataset = TwitterDataset(data_folder, img_folder, tokenizer, transform, 'train.txt')\n",
        "val_dataset = TwitterDataset(data_folder, img_folder, tokenizer, transform, 'valid.txt')\n",
        "test_dataset = TwitterDataset(data_folder, img_folder, tokenizer, transform, 'test.txt')\n",
        "\n",
        "# Data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "jaIk8uG7E71p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01bbad8f-904d-4082-e4ed-c1bef842b636"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label2id = {\n",
        "    'O': 0,\n",
        "    'B-LOC': 1, 'I-LOC': 2,\n",
        "    'B-PER': 3, 'I-PER': 4,\n",
        "    'B-ORG': 5, 'I-ORG': 6\n",
        "}\n",
        "id2label = {v: k for k, v in label2id.items()}\n",
        "\n",
        "config = BertConfig.from_pretrained('bert-base-uncased', num_labels=7)\n",
        "config.label2id = label2id\n",
        "config.id2label = id2label\n",
        "model = MTCCMBertForMMTokenClassificationCRF(config=config, num_labels=7, add_context_aware_gate=True,\n",
        "                                              use_dynamic_cross_modal_fusion=True)\n",
        "\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=5e-5)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0,\n",
        "                                            num_training_steps=len(train_loader) * 10)\n",
        "\n",
        "train_model(model, train_loader, val_loader, optimizer, scheduler, num_epochs=1)"
      ],
      "metadata": {
        "id": "ACdDNzBnGCfi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "outputId": "92743914-b48f-4e49-e368-78619c633c35"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for epoch  0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "boolean index did not match indexed array along dimension 0; dimension is 100 but corresponding boolean dimension is 38",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-e7e357fc0df3>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m                                             num_training_steps=len(train_loader) * 10)\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-e163fe199be5>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mavg_train_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_train_loss}, Val Loss: {val_loss}, Val F1: {val_f1}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-e163fe199be5>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(model, data_loader)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mvalid_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels_flattened\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mvalid_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels_flattened\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mvalid_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mall_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: boolean index did not match indexed array along dimension 0; dimension is 100 but corresponding boolean dimension is 38"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CPqctu5vEmZb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}