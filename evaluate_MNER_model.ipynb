{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UaXlK8loCpiQ"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision transformers pillow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BertCrossAttention(nn.Module):\n",
        "    \"\"\"Implements cross-attention between two different modalities using a decoder layer.\"\"\"\n",
        "\n",
        "    def __init__(self, config, num_layers):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            nn.TransformerDecoderLayer(d_model=config.hidden_size, nhead=config.num_attention_heads)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, query, key, mask=None):\n",
        "        output = query\n",
        "        for layer in self.layers:\n",
        "            output = layer(output, key, tgt_key_padding_mask=mask)\n",
        "        return output\n",
        "\n",
        "\n",
        "class MTCCMBertForMMTokenClassificationCRF(BertPreTrainedModel):\n",
        "    def __init__(self, config, num_labels):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = num_labels\n",
        "        self.bert = BertModel(config)\n",
        "        self.resnet = resnet152(pretrained=True)\n",
        "        self.resnet.fc = nn.Identity()  # Adapt ResNet to remove the final fully connected layer\n",
        "\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.vismap2text = nn.Linear(2048, config.hidden_size)\n",
        "\n",
        "        self.txt2img_attention = BertCrossAttention(config, num_layers=1)\n",
        "        self.img2txt_attention = BertCrossAttention(config, num_layers=1)\n",
        "\n",
        "        self.classifier = nn.Linear(config.hidden_size * 2, num_labels)\n",
        "        self.crf = CRF(num_labels, batch_first=True)\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, visual_embeds, labels=None):\n",
        "        # Text feature extraction\n",
        "        text_outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
        "        text_features = self.dropout(text_outputs.last_hidden_state)\n",
        "\n",
        "        # Image feature extraction with ResNet-152\n",
        "        visual_features = self.resnet(visual_embeds)  # Assuming visual_embeds is [batch_size, 3, 224, 224]\n",
        "        visual_features = visual_features.view(visual_features.size(0), -1)  # Flatten the output of the ResNet\n",
        "        visual_features = self.vismap2text(visual_features)  # Transform to match BERT hidden size\n",
        "        visual_features = visual_features.unsqueeze(1).expand(-1, text_features.size(1),\n",
        "                                                              -1)  # Expand to match text sequence length\n",
        "\n",
        "        # Cross-modal attention\n",
        "        txt_attended_visuals = self.txt2img_attention(text_features, visual_features)\n",
        "        img_attended_text = self.img2txt_attention(visual_features, text_features)\n",
        "\n",
        "        # Combine and classify\n",
        "        combined_features = torch.cat([txt_attended_visuals, img_attended_text], dim=-1)\n",
        "        logits = self.classifier(combined_features)\n",
        "\n",
        "        # crf processing\n",
        "        if labels is not None:\n",
        "            # Ensure labels and logits have the same sequence length\n",
        "            labels = torch.where(labels == -100, torch.zeros_like(labels), labels)\n",
        "\n",
        "            seq_length = logits.size(1)\n",
        "            if labels.size(1) < seq_length:\n",
        "                padding_size = seq_length - labels.size(1)\n",
        "                # Use a valid label index for padding, e.g., 0\n",
        "                labels_padded = torch.full((labels.size(0), padding_size), fill_value=0, dtype=torch.long,\n",
        "                                           device=labels.device)\n",
        "                labels = torch.cat([labels, labels_padded], dim=1)\n",
        "\n",
        "                # Adjust attention_mask to cover only the non-padded areas\n",
        "                attention_mask_padded = torch.zeros((attention_mask.size(0), seq_length), dtype=torch.uint8,\n",
        "                                                    device=attention_mask.device)\n",
        "                attention_mask_padded[:, :attention_mask.size(1)] = attention_mask\n",
        "                attention_mask = attention_mask_padded\n",
        "\n",
        "            # CRF loss calculation\n",
        "            loss = -self.crf(logits, labels, mask=attention_mask.byte(), reduction='mean')\n",
        "            return loss\n",
        "        else:\n",
        "            return self.crf.decode(logits, mask=attention_mask.byte())\n"
      ],
      "metadata": {
        "id": "itprnvRVC3bA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(file_path, image_dir):\n",
        "    texts, labels, image_paths = [], [], []\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) == 3:\n",
        "                text, label, img_id = parts\n",
        "                texts.append(text)\n",
        "                labels.append(label)\n",
        "                image_paths.append(f\"{image_dir}/{img_id}.jpg\")\n",
        "    return texts, labels, image_paths\n",
        "\n",
        "# Evaluate on a test dataset\n",
        "def evaluate_model(model, texts, labels, image_paths):\n",
        "    predictions = []\n",
        "    for text, img_path in zip(texts, image_paths):\n",
        "        input_ids, attention_mask, image = prepare_input(text, img_path)\n",
        "        with torch.no_grad():\n",
        "            logits = model(input_ids, attention_mask, image)\n",
        "            prediction = model.crf.decode(logits)\n",
        "        predictions.append(prediction)\n",
        "    # Here you might want to compare predictions with true labels, compute accuracy, etc.\n",
        "    return predictions\n",
        "\n",
        "def demo_prediction(text, image_path):\n",
        "    input_ids, attention_mask, image = prepare_input(text, image_path)\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_ids, attention_mask, image)\n",
        "        prediction_indices = model.crf.decode(logits)\n",
        "        predicted_tags = [id2label[idx] for idx in prediction_indices[0]]  # Convert indices to labels\n",
        "\n",
        "    # Show the image and prediction\n",
        "    img = Image.open(image_path)\n",
        "    plt.imshow(img)\n",
        "    plt.title(f\"Predicted Tags: {predicted_tags}\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Z0ik9UK3DLJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = MTCCMBertForMMTokenClassificationCRF(config=config, num_labels=7)\n",
        "model.load_state_dict(torch.load('path_to_model.pth'))\n",
        "model.eval()\n",
        "\n",
        "# Image transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Dummy paths\n",
        "texts, true_labels, image_paths = load_data(\"test.txt\", \"path_to_images\")\n",
        "predictions = evaluate_model(model, texts, true_labels, image_paths)"
      ],
      "metadata": {
        "id": "6QroxUtNCwvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    logits = model(input_ids, attention_mask, image)\n",
        "    predictions = model.crf.decode(logits)  # Get tag indices\n",
        "\n",
        "def decode_tags(tag_indices, id2label):\n",
        "    # Convert tag indices to tag names\n",
        "    return [id2label[idx] for idx in tag_indices[0]]  # Assuming batch size of 1\n",
        "\n",
        "# Assuming id2label mapping is available\n",
        "predicted_tags = decode_tags(predictions, id2label)\n",
        "print(predicted_tags)\n"
      ],
      "metadata": {
        "id": "SyfojElwC7PL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}